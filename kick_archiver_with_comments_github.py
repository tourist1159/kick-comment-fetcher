import json
import os
import time
from datetime import datetime, timedelta, timezone
from urllib.request import Request, urlopen
from urllib.error import HTTPError, URLError
from urllib.parse import quote
import sys
import functools
from pprint import pprint

# ã™ã¹ã¦ã® print() ã‚’ stderr ã«å‡ºã™
print = functools.partial(print, file=sys.stderr, flush=True)

# === ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã™ã‚‹åŸºæº–æ—¥æ™‚ï¼ˆã“ã“ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰ ===
USER_START_DATE = "2025-11-06T00:00:00+09:00"

# === è¨­å®š ===
CHANNEL_ID = "56495977"
CHANNEL_NAME = "mokoutoaruotoko"
API_URL = f"https://kick.com/api/v2/channels/{CHANNEL_NAME}/videos"
# ä¿å­˜ãƒ•ã‚©ãƒ«ãƒ€è¨­å®š
COMMENTS_GITHUB = "comments_github"
COMMENTS_LOCAL = "comments_local"
ARCHIVE_FILE = "kick_archives.json"

# ãƒ­ãƒ¼ã‚«ãƒ«å„ªå…ˆ
def get_comment_dir():
    return COMMENTS_LOCAL if os.path.exists(COMMENTS_LOCAL) else COMMENTS_GITHUB

# === ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ===
def to_iso(dt_str):
    """Kickã®created_atã‚’ISOå½¢å¼ã«çµ±ä¸€"""
    if not dt_str:
        return None
    try:
        newdt = dt_str.replace(" ", "T")
        if (not "Z" in newdt ): newdt = newdt+"Z"
        return datetime.fromisoformat(newdt).isoformat()
    except Exception:
        return None


def format_duration(ms):
    """ãƒŸãƒªç§’ã‚’ HH:MM:SS ã«æ•´å½¢"""
    try:
        s = int(ms) // 1000
        return time.strftime("%H:%M:%S", time.gmtime(s))
    except Exception:
        return "00:00:00"
    
    
def compute_timeinfo(video):
    start_time_iso = video["start_time"]
    start_time_dt = datetime.fromisoformat(start_time_iso)
    duration = video["duration"]
    end_time_dt = start_time_dt + timedelta(milliseconds=duration)
    return start_time_iso, start_time_dt, end_time_dt


# === ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å–å¾— ===
def fetch_archives(max_retries=3):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0.0.0 Safari/537.36"
        ),
        "Accept": "application/json, text/plain, */*",
        "Referer": "https://kick.com/",
        "Accept-Language": "en-US,en;q=0.9",
        "Connection": "keep-alive",
    }

    for attempt in range(max_retries):
        try:
            req = Request(API_URL, headers=headers)
            with urlopen(req, timeout=15) as response:
                if response.status != 200:
                    print(f"HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {response.status}")
                    continue
                raw = response.read().decode("utf-8")
                data = json.loads(raw)
                formatted = []
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸåŸºæº–æ—¥æ™‚ã‚’ UTC ã«å¤‰æ›        
                user_start_dt = datetime.fromisoformat(USER_START_DATE).astimezone(timezone.utc)    
                for v in data:
                    if v.get("is_live"): continue
                    t = to_iso(v.get("start_time"))
                    # æŒ‡å®šæ—¥æ™‚ä»¥é™ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®ã¿å¯¾è±¡
                    if datetime.fromisoformat(t) < user_start_dt: continue
                    formatted.append({
                        "id": v.get("id"),
                        "video_id": v.get("video", {}).get("id"),
                        "uuid": v.get("video", {}).get("uuid"),
                        "title": v.get("session_title") or "",
                        "start_time": t,
                        "url": f"https://kick.com/{CHANNEL_NAME}/videos/{v.get('video', {}).get('uuid')}",
                        "duration": v.get("duration"),
                        "video_length":format_duration(v.get("duration")),
                    })
                return formatted

        except HTTPError as e:
            print(f"[{attempt+1}/{max_retries}] HTTPã‚¨ãƒ©ãƒ¼: {e.code}")
            time.sleep(3)
        except URLError as e:
            print(f"[{attempt+1}/{max_retries}] URLã‚¨ãƒ©ãƒ¼: {e.reason}")
            time.sleep(3)
        except Exception as e:
            print(f"[{attempt+1}/{max_retries}] ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼: {e}")
            time.sleep(3)

    print("Kick APIã‚¢ã‚¯ã‚»ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    return []

# ---------- ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ç®¡ç† ----------
def load_local_archives():
    if os.path.exists(ARCHIVE_FILE):
        with open(ARCHIVE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# === ã‚³ãƒ¡ãƒ³ãƒˆå–å¾— ===
def get_chat_messages(start_time_iso):
    """æŒ‡å®šæ™‚åˆ»ä»¥é™ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—"""
    start_time_encoded = quote(start_time_iso, safe="")
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0.0.0 Safari/537.36"
        ),
        "Accept": "application/json, text/plain, */*",
        "Referer": "https://kick.com/",
        "Accept-Language": "en-US,en;q=0.9",
        "Connection": "keep-alive",
    }
    url = f"https://kick.com/api/v2/channels/{CHANNEL_ID}/messages?start_time={start_time_encoded}"

    try:
        req = Request(url, headers=headers)
        with urlopen(req, timeout=15) as res:
            data = json.loads(res.read().decode("utf-8"))
            msg = data.get("data", {}).get("messages", [])
            return msg
    except HTTPError as e:
        print(f"HTTPã‚¨ãƒ©ãƒ¼: {e.code} {url}")
    except URLError as e:
        print(f"URLã‚¨ãƒ©ãƒ¼: {e.reason}")
    except Exception as e:
        print(f"ã‚³ãƒ¡ãƒ³ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    return []


def get_all_comments(start_time_iso, start_time, end_time):
    """é…ä¿¡å…¨ä½“ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—"""
    all_comments = []
    current = start_time
    current_iso = start_time_iso

    while current < end_time:
        print(f"å–å¾—ä¸­: {current}/{end_time}")
        messages = get_chat_messages(current_iso)
        if not messages:
            current += timedelta(seconds=5)
            current_iso = current.isoformat()
            time.sleep(1)
            continue

        for msg in messages:
            id=msg['user_id']
            t=msg.get('created_at')
            c=msg.get('content') or ''
            all_comments.append({"id": id, "timestamp": t, "text": c})
            
        last_time = messages[-1].get("created_at")
        if not last_time:
            break
        current = datetime.fromisoformat(last_time) + timedelta(seconds=1)
        current_iso = current.isoformat()
        time.sleep(1)

    return all_comments


def save_comment_stats(video, comments):
    comment_dir = get_comment_dir()
    if not comments:
        print(f"ã‚³ãƒ¡ãƒ³ãƒˆãªã—: {video['id']}")
        return

    try:

        data = {
            "video_id": video["id"],
            "start_time": video["start_time"],
            "video_length": video["video_length"],
            "number_of_comments": video["number_of_comments"],
            "comments": comments,
        }

        path = os.path.join(comment_dir, f"{video['id']}_comments.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ã‚³ãƒ¡ãƒ³ãƒˆçµ±è¨ˆä¿å­˜: {path}")

    except Exception as e:
        print(f"çµ±è¨ˆä¿å­˜ã‚¨ãƒ©ãƒ¼({video['id']}): {e}")
        
# kick_archives.jsonã‚’æ›´æ–°
def update_archive_data(archives):
    with open(ARCHIVE_FILE, "w", encoding="utf-8") as f:
        json.dump(archives, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ {ARCHIVE_FILE} æ›´æ–°å®Œäº†")

# å¤ã„ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ï¼ˆGitHubãƒ•ã‚©ãƒ«ãƒ€ã®ã¿ï¼‰
def cleanup_old_comments():
    limit = datetime.now(timezone.utc) - timedelta(days=7)
    for f in os.listdir(COMMENTS_GITHUB):
        if not f.endswith("_comments.json"):
            continue
        path = os.path.join(COMMENTS_GITHUB, f)
        mtime = datetime.fromtimestamp(os.path.getmtime(path), tz=timezone.utc)
        if mtime < limit:
            os.remove(path)
            print(f"ğŸ§¹ å¤ã„ã‚³ãƒ¡ãƒ³ãƒˆå‰Šé™¤: {f}")



# === ãƒ¡ã‚¤ãƒ³ ===
def main():
    try:
        print("Fetching archive list...")
        local_archives = load_local_archives()
        known_ids = {a["id"] for a in local_archives}
        remote_archives = fetch_archives()    

        new_archives = [a for a in remote_archives if a["id"] not in known_ids]
        if not new_archives:
            print("æ–°ã—ã„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
            return

        for video in new_archives:
            print(f"æ–°ã—ã„ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–: {video['title']} ({video['id']})")
            start_time_iso, start_time_dt, end_time_dt = compute_timeinfo(video)
            comments = get_all_comments(start_time_iso, start_time_dt, end_time_dt)
            video['number_of_comments'] = len(comments)
            save_comment_stats(video, comments)
            local_archives.append(video)
            time.sleep(3)

        update_archive_data(local_archives)
        
        cleanup_old_comments()

    except Exception as e:
        print(f"å®Ÿè¡Œä¸­ã‚¨ãƒ©ãƒ¼: {e}")


if __name__ == "__main__":
    main()
